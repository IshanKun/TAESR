{
  "architecture": "mlp_mixer",
  "layers": 1,
  "hidden_dim": 128,
  "max_seq_length": 128,
  "vocab_size": 20000,
  "max_recursion_steps": 2,
  "use_early_exit": true,
  "early_exit_threshold": 0.85,
  "learning_rate": 0.0002,
  "weight_decay": 0.2,
  "ema_decay": 0.99,
  "batch_size": 256,
  "max_epochs": 30,
  "warmup_steps": 500,
  "use_gradient_checkpointing": false,
  "mixed_precision": false,
  "cache_size": 5000,
  "enable_monitoring": false
}