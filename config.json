{
  "architecture": "self_attention",
  "layers": 2,
  "hidden_dim": 128,
  "max_seq_length": 64,
  "vocab_size": 5000,
  "max_recursion_steps": 4,
  "use_early_exit": true,
  "early_exit_threshold": 0.98,
  "learning_rate": 0.0001,
  "weight_decay": 0.1,
  "ema_decay": 0.999,
  "batch_size": 32,
  "max_epochs": 100,
  "warmup_steps": 2000,
  "use_gradient_checkpointing": false,
  "mixed_precision": false,
  "cache_size": 1000,
  "enable_monitoring": true
}